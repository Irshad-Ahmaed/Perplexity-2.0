from typing import TypedDict, Annotated, Optional
from langgraph.graph import add_messages, StateGraph, END
from langchain.agents import initialize_agent, tool
from langchain.tools import Tool
from langchain_together import ChatTogether
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.checkpoint.memory import MemorySaver
from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from langchain_core.messages import AIMessage, AIMessageChunk, HumanMessage, ToolMessage
from uuid import uuid4
from dotenv import load_dotenv
from pydantic import BaseModel
import datetime
import json
import os

# Load environment variables
load_dotenv()

llm = ChatTogether(
    model="mistralai/Mixtral-8x7B-Instruct-v0.1",
    temperature=0.7,
    together_api_key=os.getenv("TOGETHER_API_KEY"),
    streaming=True
)

# Tool definitions
@tool
def get_system_time(format: str = "%Y-%m-%d %H:%M:%S"):
    """Returns the current date and time in the specific format"""
    return datetime.datetime.now().strftime(format)

@tool
def calculate_days_between(input: str) -> int:
    """Calculate the number of days between two dates in 'YYYY-MM-DD to YYYY-MM-DD' format."""
    try:
        parts = input.strip().replace('"', '').replace("'", "").split(" to ")
        if len(parts) != 2:
            return "Input must be in format: 'YYYY-MM-DD to YYYY-MM-DD'"
        start_date = datetime.datetime.strptime(parts[0].strip(), "%Y-%m-%d")
        end_date = datetime.datetime.strptime(parts[1].strip(), "%Y-%m-%d")
        return (end_date - start_date).days
    except Exception as e:
        return f"Error parsing input: {e}"

search_tool = TavilySearchResults(search_depth='basic')
tools = [search_tool, get_system_time, calculate_days_between]
memory = MemorySaver()

llm_with_tools = initialize_agent(tools=tools, llm=llm, agent="zero-shot-react-description", verbose=True)

class State(TypedDict):
    messages: Annotated[list, add_messages]

async def model(state: State):
    result = await llm_with_tools.ainvoke(state["messages"])
    return {"messages": state["messages"] + [AIMessage(content=result["output"])]}

async def tools_router(state: State):
    last_message = state["messages"][-1]
    if hasattr(last_message, "tool_calls") and len(last_message.tool_calls) > 0:
        return "tool_node"
    return "end"

async def tool_node(state):
    tool_calls = state["messages"][-1].tool_calls
    tool_messages = []
    for call in tool_calls:
        tool_name, tool_args, tool_id = call["name"], call["args"], call["id"]
        if tool_name == search_tool.name:
            result = await search_tool.ainvoke(tool_args)
            message = ToolMessage(
                content=str(result), tool_call_id=tool_id, name=tool_name
            )
            tool_messages.append(message)
    return {"messages": state["messages"] + tool_messages}

# LangGraph setup
graph_builder = StateGraph(State)
graph_builder.add_node("model", model)
graph_builder.add_node("tool_node", tool_node)
graph_builder.set_entry_point("model")
graph_builder.add_edge("tool_node", "model")
graph_builder.add_conditional_edges("model", tools_router, {"tool_node": "tool_node", "end": END})
graph = graph_builder.compile(checkpointer=memory)

# FastAPI setup
app = FastAPI()
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

def serialize_ai_message_chunk(chunk):
    return chunk.content if isinstance(chunk, AIMessageChunk) else str(chunk)

async def generate_chat_responses(message: str, checkpoint_id: Optional[str] = None):
    is_new = checkpoint_id is None
    new_id = str(uuid4()) if is_new else checkpoint_id
    config = {"configurable": {"thread_id": new_id}}

    if is_new:
        yield f"data: {{\"type\": \"checkpoint\", \"checkpoint_id\": \"{new_id}\"}}\n\n"

    events = graph.astream_events({"messages": [HumanMessage(content=message)]}, version="v2", config=config)

    async for event in events:
        etype = event["event"]

        if etype == "on_chat_model_stream":
            content = serialize_ai_message_chunk(event["data"]["chunk"]).replace("\n", "\\n")
            yield f"data: {{\"type\": \"content\", \"content\": \"{content}\"}}\n\n"
        
        elif etype == "on_chat_model_end":
            output_text = event["data"]["output"].content if hasattr(event["data"]["output"], "content") else ""

            if "tavily_search_results_json" in output_text:
                import re
                match = re.search(r'Action Input:\s*(\{.*?\})', output_text, re.DOTALL)
                if match:
                    try:
                        parsed = json.loads(match.group(1))
                        search_query = parsed.get("query", "")
                        safe_query = search_query.replace('"', '\\"').replace("'", "\\'").replace("\n","\\n")
                        yield f"data: {{\"type\": \"search_start\", \"query\": \"{safe_query}\"}}\n\n"
                    except json.JSONDecodeError:
                        pass


        elif etype == "on_tool_end":
            output = event["data"]["output"]
            if isinstance(output, list):
                urls = [item["url"] for item in output if isinstance(item, dict) and "url" in item]
                yield f"data: {{\"type\": \"search_results\", \"urls\": {json.dumps(urls)} }}\n\n"

    yield "data: {\"type\": \"end\"}\n\n"

@app.get("/chat_stream/{message}")
async def chat_stream(message: str, checkpoint_id: Optional[str] = Query(None)):
    return StreamingResponse(generate_chat_responses(message, checkpoint_id), media_type="text/event-stream")