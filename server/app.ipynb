{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126e8051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Optional # Using for defining the state of our agent graph\n",
    "from langgraph.graph import add_messages, StateGraph, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import initialize_agent, tool, AgentType\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import Tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from uuid import uuid4\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# LangChain-compatible Gemini model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556c6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "def search_web(query: str):\n",
    "    search_results = TavilySearchResults(max_results=2).invoke({\"query\": query})\n",
    "\n",
    "    if not search_results:\n",
    "        return \"No relevant search results found.\"\n",
    "\n",
    "    return search_results\n",
    "\n",
    "class SearchToolSchema(BaseModel):\n",
    "    query: str\n",
    "\n",
    "# search_tool = Tool.from_function(\n",
    "#     func=search_web,\n",
    "#     name=\"tavily_Search\",\n",
    "#     description=\"Retrieve real-time information like weather, news, or current events using web search.\",\n",
    "#     args_schema= SearchToolSchema\n",
    "# ) # Find only given number of result\n",
    "@tool\n",
    "def get_system_time(format: str = \"%Y-%m-%d %H:%M:%S\"):\n",
    "    \"\"\"Returns the current date and time in the specific format\"\"\"\n",
    "    current_time = datetime.datetime.now()\n",
    "    formatted_time = current_time.strftime(format)\n",
    "    return formatted_time\n",
    "\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculate_days_between(input: str) -> int:\n",
    "    \"\"\"Calculate the number of days between two dates in 'YYYY-MM-DD to YYYY-MM-DD' format.\"\"\"\n",
    "    from datetime import datetime\n",
    "    try:\n",
    "        # Clean and split\n",
    "        parts = input.strip().replace('\"', '').replace(\"'\", \"\").split(\" to \")\n",
    "        if len(parts) != 2:\n",
    "            return \"Input must be in format: 'YYYY-MM-DD to YYYY-MM-DD'\"\n",
    "        \n",
    "        start_date = parts[0].strip()\n",
    "        end_date = parts[1].strip()\n",
    "\n",
    "        fmt = \"%Y-%m-%d\"\n",
    "        delta = datetime.strptime(end_date, fmt) - datetime.strptime(start_date, fmt)\n",
    "        return delta.days\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error parsing input: {e}\"\n",
    "\n",
    "search_tool = TavilySearchResults(search_depth='basic') # Find only given number of result\n",
    "\n",
    "tools = [search_tool, get_system_time, calculate_days_between] # storing search tool in general tool array\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_with_tools = llm.bind_tools(tools=tools)\n",
    "# llm_with_tools = initialize_agent(tools=tools, llm=llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "llm_with_tools = initialize_agent(tools=tools, llm=llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "# llm_with_tools = initialize_agent(tools=tools, llm=llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True)\n",
    "# response = llm_with_tools.invoke(\"What's the todays weather of Gwalior India\")\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec2011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "async def model(state: State):\n",
    "    result = await llm_with_tools.ainvoke(state[\"messages\"])\n",
    "    return{\n",
    "        \"messages\": state[\"messages\"] + [AIMessage(content=result[\"output\"])]\n",
    "    }\n",
    "\n",
    "async def tools_router(state: State):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "\n",
    "    if(hasattr(last_message, \"tool_calls\") and len(last_message.tool_calls) > 0):\n",
    "        return \"tool_node\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "    \n",
    "async def tool_node(state):\n",
    "    \"\"\"Custom tool node that handles tool calls from the LLM.\"\"\"\n",
    "    # Get the tool calls from the last message\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "\n",
    "    # Initialize list to store tool messages\n",
    "    tool_messages = []\n",
    "\n",
    "    # Process each tool call\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call[\"name\"]\n",
    "        tool_args = tool_call[\"args\"]\n",
    "        tool_id = tool_call[\"id\"]\n",
    "        \n",
    "        # Handle the search tool\n",
    "        if tool_name == search_tool.name:\n",
    "            # Execute the search tool with the provided arguments\n",
    "            search_results = await search_tool.ainvoke(tool_args)\n",
    "\n",
    "            # Create a ToolMessage for this result\n",
    "            tool_message = ToolMessage(\n",
    "                content = str(search_results),\n",
    "                tool_call_id=tool_id,\n",
    "                name = tool_name\n",
    "            )\n",
    "\n",
    "            tool_messages.append(tool_message)\n",
    "\n",
    "    # Add the tool messages to the state\n",
    "    return {\n",
    "        \"messages\": tool_messages\n",
    "    }\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "graph_builder.add_node(\"model\", model)\n",
    "graph_builder.add_node(\"tool_node\", tool_node)\n",
    "graph_builder.set_entry_point(\"model\")\n",
    "\n",
    "graph_builder.add_edge(\"tool_node\", \"model\")\n",
    "graph_builder.add_conditional_edges(\"model\", tools_router, {\"tool_node\": \"tool_node\", \"end\": END})\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3d5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435410c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\n",
    "#     \"configurable\":{\n",
    "#         \"thread_id\": 123\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# response = await graph.ainvoke({\n",
    "#     \"messages\": [HumanMessage(content=\"When is the last spaceX launch and how many days have passed from today?\")]\n",
    "# }, config=config)\n",
    "\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3599f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"configurable\":{\n",
    "        \"thread_id\": 130\n",
    "    }\n",
    "}\n",
    "\n",
    "# Use async to iterate over the async generator\n",
    "async for event in graph.astream_events({\n",
    "    \"messages\": [HumanMessage(content=\"Hi I'm Irshad, how are you?\")]\n",
    "}, config=config, version=\"v2\"):\n",
    "    if event[\"event\"]:\n",
    "        print(event)\n",
    "# async for event in graph.astream_events({\n",
    "#     \"messages\": [HumanMessage(content=\"Give me poem for kids in 150 words\")]\n",
    "# }, config=config, version=\"v2\"):\n",
    "#     if event[\"event\"] == \"on_chat_model_stream\":\n",
    "#         print(event[\"data\"][\"chunk\"].content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
