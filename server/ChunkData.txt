# from typing import TypedDict, Annotated, Optional
# from langgraph.graph import add_messages, StateGraph, END
# from langchain.tools import Tool, tool
# from langchain_community.tools.tavily_search import TavilySearchResults
# from langgraph.checkpoint.memory import MemorySaver
# from fastapi import FastAPI, Query, Request
# from fastapi.middleware.cors import CORSMiddleware
# from fastapi.responses import StreamingResponse, JSONResponse
# from langchain_core.messages import AIMessage, AIMessageChunk, HumanMessage, ToolMessage
# from langchain_google_genai import ChatGoogleGenerativeAI
# from pydantic import BaseModel
# from uuid import uuid4
# from dotenv import load_dotenv
# import datetime
# import json
# import asyncio
# import tenacity
# import os

# load_dotenv()

# # Gemini LLM with streaming and tool support
# llm = ChatGoogleGenerativeAI(
#     model="gemini-1.5-flash",
#     temperature=0.7,
#     model_kwargs={"streaming": True}
# )

# # --- Tool Definitions ---
# class SearchToolSchema(BaseModel):
#     query: str

# def search_web(query: str):
#     return TavilySearchResults(max_results=2).invoke({"query": query})

# search_tool = Tool.from_function(
#     func=search_web,
#     name="search",
#     description="Use this to search real-time info from the web, like weather, news, events, and launch dates. Useful for answering anything the model might not know or that's updated frequently (e.g. 'next SpaceX launch', 'today's temperature in Delhi').",
#     args_schema=SearchToolSchema
# )

# @tool
# def get_system_time(format: str = "%Y-%m-%d %H:%M:%S") -> str:
#     """Returns the current system time in the specified format."""
#     return datetime.datetime.now().strftime(format)

# @tool
# def calculate_days_between(input: str) -> str:
#     """Calculate number of days between 'YYYY-MM-DD to YYYY-MM-DD'."""
#     try:
#         parts = input.strip().replace('"', '').replace("'", '').split(" to ")
#         if len(parts) != 2:
#             return "Invalid format. Use: 'YYYY-MM-DD to YYYY-MM-DD'"
#         start = datetime.datetime.strptime(parts[0].strip(), "%Y-%m-%d")
#         end = datetime.datetime.strptime(parts[1].strip(), "%Y-%m-%d")
#         return str((end - start).days) + " days"
#     except Exception as e:
#         return f"Error: {e}"

# # Retry tool call
# @tenacity.retry(wait=tenacity.wait_fixed(1), stop=tenacity.stop_after_attempt(3))
# async def safe_tool_invoke(tool, args):
#     return await asyncio.to_thread(tool.invoke, args)


# # --- Bind tools ---
# tools = [search_tool, get_system_time, calculate_days_between]
# llm_with_tools = llm.bind_tools(tools=tools)

# # --- LangGraph Setup ---
# class State(TypedDict):
#     messages: Annotated[list, add_messages]

# memory = MemorySaver()

# def trim_messages(messages: list, max_len: int = 12):
#     return messages[-max_len:] if len(messages) > max_len else messages

# async def model(state: State):
#     """Stream responses word-by-word from Gemini or return fallback."""
#     try:
#         last_msg = state["messages"][-1]

#         if not state["messages"] or not getattr(last_msg, "content", "").strip():
#             yield {"messages": state["messages"] + [AIMessage(content="No valid query provided.")]}

#         # Stream from Gemini
#         stream = llm_with_tools.astream(state["messages"])
#         chunks = []
#         async for chunk in stream:
#             if isinstance(chunk, AIMessageChunk):
#                 yield {"messages": state["messages"] + [chunk]}
#                 chunks.append(chunk)

#         # Final full message (if needed for state saving)
#         final = AIMessage(content="".join(c.content for c in chunks if isinstance(c, AIMessageChunk)))
#         yield {"messages": state["messages"] + [final]}
#     except Exception as e:
#         print("Error in model node:", e)
#         yield {"messages": state["messages"] + [AIMessage(content="Sorry, an error occurred while generating a response.")]}

# async def tools_router(state: State):
#     try:
#         last_msg = state["messages"][-1]
#         if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
#             return "tool_node"
#     except Exception as e:
#         print("Error in tools_router:", e)
#     return "end"

# async def tool_node(state: State):
#     try:
#         tool_calls = state["messages"][-1].tool_calls
#         tool_messages = []
#         for call in tool_calls:
#             name, args, tool_id = call["name"], call["args"], call["id"]
#             tool_obj = next((t for t in tools if t.name == name), None)
#             if tool_obj:
#                 try:
#                     output = tool_obj.invoke(args)
#                     tool_messages.append(ToolMessage(content=str(output), tool_call_id=tool_id))
#                 except Exception as e:
#                     tool_messages.append(ToolMessage(content=f"Error running tool '{name}': {e}", tool_call_id=tool_id))
#         yield {"messages": state["messages"] + tool_messages}
#     except Exception as e:
#         print("Error in tool_node:", e)
#         yield {"messages": state["messages"] + [AIMessage(content="Tool execution failed.")]}

# # Build LangGraph
# graph_builder = StateGraph(State)
# graph_builder.add_node("model", model)
# graph_builder.add_node("tool_node", tool_node)
# graph_builder.set_entry_point("model")
# graph_builder.add_edge("tool_node", "model")
# graph_builder.add_conditional_edges("model", tools_router, {"tool_node": "tool_node", "end": END})
# graph = graph_builder.compile(checkpointer=memory)

# # --- FastAPI Setup ---
# app = FastAPI()
# app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

# @app.middleware("http")
# async def validate_length(request: Request, call_next):
#     body = await request.body()
#     if len(body) > 5000:
#         return JSONResponse(status_code=413, content={"error": "Request body too large."})
#     return await call_next(request)

# def serialize_chunk(chunk):
#     return chunk.content if isinstance(chunk, AIMessageChunk) else str(chunk)

# async def generate_chat_responses(message: str, checkpoint_id: Optional[str] = None):
#     try:
#         is_new = checkpoint_id is None
#         thread_id = str(uuid4()) if is_new else checkpoint_id

#         if is_new:
#             yield f"data: {{\"type\": \"checkpoint\", \"checkpoint_id\": \"{thread_id}\"}}\n\n"

#         config = {"configurable": {"thread_id": thread_id}}
#         events = graph.astream_events({"messages": [HumanMessage(content=message)]}, version="v2", config=config)

#         async for event in events:
#             etype = event["event"]

#             if etype == "on_chat_model_stream":
#                 chunk = event["data"]["chunk"]
#                 content = chunk.content.replace("\n", "\\n").replace('"', '\\"')
#                 for word in content.split(" "):
#                     yield f"data: {{\"type\": \"content\", \"content\": \"{word} \"}}\n\n"

#             elif etype == "on_chat_model_end":
#                 tool_calls = getattr(event["data"]["output"], "tool_calls", [])
#                 for call in tool_calls:
#                     if call["name"] == "search":
#                         query = call["args"].get("query", "")
#                         safe_query = query.replace('"', '\\"').replace("'", "\\'").replace("\n", "\\n")
#                         yield f"data: {{\"type\": \"search_start\", \"query\": \"{safe_query}\"}}\n\n"

#             elif etype == "on_tool_end" and event["name"] == "search":
#                 output = event["data"]["output"]
#                 if isinstance(output, list):
#                     urls = [item["url"] for item in output if isinstance(item, dict) and "url" in item]
#                     urls_json = json.dumps(urls)
#                     yield f"data: {{\"type\": \"search_results\", \"urls\": {urls_json}}}\n\n"

#         yield f"data: {{\"type\": \"end\"}}\n\n"

#     except Exception as e:
#         print("Error in generate_chat_responses:", e)
#         yield f"data: {{\"type\": \"error\", \"message\": \"{str(e)}\"}}\n\n"

# @app.get("/chat_stream/{message}")
# async def chat_stream(message: str, checkpoint_id: Optional[str] = Query(None)):
#     return StreamingResponse(generate_chat_responses(message, checkpoint_id), media_type="text/event-stream")
